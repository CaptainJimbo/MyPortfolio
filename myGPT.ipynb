{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKA2ZMWu4Y3qkP0yNAPoUg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CaptainJimbo/MyPortfolio/blob/main/myGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Based on \"Attention is all you need\" paper** [(link)](https://arxiv.org/abs/1706.03762). This simple algorithm is a Transformer-based Language Model to showcase how an LLM like ChatGPT is trained. It doens't include the pretuning and supervised finetuning."
      ],
      "metadata": {
        "id": "1iXTpB2XrboY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There are two big open-source libraries for deep learning Tensorflow and Torch. I 'll use torch.\n",
        "import torch"
      ],
      "metadata": {
        "id": "qmYmUmRn1Cw5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I need a \"toy\" dataset to train with.\n",
        "# (This is very small comparing to a big chunk of the internet that ChatGPT is trained on!)\n",
        "# This is a .txt file with some of Shakespeare's works.\n",
        "# The goal is to create a model that produces Shakespearean language!!\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSelmmzys3HD",
        "outputId": "d984b319-5db4-4733-d1ed-82b262db3ed7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-13 07:55:19--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-07-13 07:55:20 (19.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(f'length of the dataset is {len(text)}')\n",
        "print(f'\\nand here is a random part of the dataset {text[60:464]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8_Xn7AvuuhS",
        "outputId": "13a1fc23-db01-4984-82ce-4e264909d2a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the dataset is 1115394\n",
            "\n",
            "and here is a random part of the dataset \n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The algorithm needs to understands characters. But it doesn't need the the particular characters.\n",
        "# It could be numbers i.e. indices. So I create a mapping from characters to indices.\n",
        "vocabulary = sorted(list(set(text)))\n",
        "print('This is the vocabulary of the text, i.e. every possible character that exists in this text.',''.join(vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73xXmJxBv2Mp",
        "outputId": "b2747fc0-dba3-4f78-f48b-0f5e552415d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the vocabulary of the text, i.e. every possible character that exists in this text. \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the mapping from characters to indices and vice verca.\n",
        "char_to_idx = {character:index for index, character in enumerate(vocabulary)}\n",
        "idx_to_char = {index:character for index, character in enumerate(vocabulary)}\n",
        "\n",
        "# And functions for easier handling.\n",
        "def encode(text):\n",
        "  return [char_to_idx[character] for character in text]\n",
        "def decode(indices):\n",
        "  return ''.join(idx_to_char[index] for index in indices)\n",
        "\n",
        "#encode('Hello There'), decode(encode('Hello There'))"
      ],
      "metadata": {
        "id": "cvE_L2Eav3-8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text),dtype=torch.long) # This is tensor with indices representing characters.\n",
        "print('tensor shape',data.shape)\n",
        "print('tensor  type',data.dtype)\n",
        "print('tensor  rank',data.dim())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bapFjCXzyclQ",
        "outputId": "3c8a54fd-65e7-4945-c11e-dd851eec4418"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor shape torch.Size([1115394])\n",
            "tensor  type torch.int64\n",
            "tensor  rank 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a train set and a test set.\n",
        "train_data = data[:int(0.8*len(data))]\n",
        "test_data = data[int(0.8*len(data)):]"
      ],
      "metadata": {
        "id": "-Pw594md1MyV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gram_len = 5\n",
        "X = train_data[:gram_len]\n",
        "y = train_data[1:gram_len+1]\n",
        "X, y, y[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDYZ_TkdlaYJ",
        "outputId": "7afd204e-4100-4c35-deb4-435667de7ff1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([18, 47, 56, 57, 58]), tensor([47, 56, 57, 58,  1]), tensor(1))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "BATCH_SIZE = 4\n",
        "BLOCK_SIZE = 8\n",
        "\n",
        "def get_batch(type, batch_size, block_size):\n",
        "    data = train_data if type=='train' else test_data\n",
        "    inits = torch.randint(len(data)-block_size, (batch_size,))\n",
        "    X = torch.stack([data[i:i+block_size] for i in inits])\n",
        "    Y = torch.stack([data[i+1:i+block_size+1] for i in inits])\n",
        "    return X,Y\n",
        "\n",
        "B = 4\n",
        "T = 8\n",
        "x_batch, y_batch = get_batch('train',B, T) # Get 4 8-grams!"
      ],
      "metadata": {
        "id": "jpzG6okKluPT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'I have to train a transformer so that when it is feeded {x_batch[0]} \\n\\\n",
        "it will look for the correct desired targets as {y_batch[0]}')"
      ],
      "metadata": {
        "id": "PZCDvoOOsLQg",
        "outputId": "645aca1d-5be7-4e04-9309-801d984bb500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have to train a transformer so that when it is feeded tensor([58, 63,  8,  0,  0, 19, 24, 27]) \n",
            "it will look for the correct desired targets as tensor([63,  8,  0,  0, 19, 24, 27, 33])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B = 4\n",
        "T = 8\n",
        "for b in range(B): # batch dimension\n",
        "    for t in range(T): # time dimension\n",
        "        context = x_batch[b, :t+1]\n",
        "        target = y_batch[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "ISlPGUOvpXpN",
        "outputId": "3d1ba569-a6c0-480a-b634-49c74e5fda2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is [58] the target: 63\n",
            "when input is [58, 63] the target: 8\n",
            "when input is [58, 63, 8] the target: 0\n",
            "when input is [58, 63, 8, 0] the target: 0\n",
            "when input is [58, 63, 8, 0, 0] the target: 19\n",
            "when input is [58, 63, 8, 0, 0, 19] the target: 24\n",
            "when input is [58, 63, 8, 0, 0, 19, 24] the target: 27\n",
            "when input is [58, 63, 8, 0, 0, 19, 24, 27] the target: 33\n",
            "when input is [39] the target: 59\n",
            "when input is [39, 59] the target: 45\n",
            "when input is [39, 59, 45] the target: 46\n",
            "when input is [39, 59, 45, 46] the target: 58\n",
            "when input is [39, 59, 45, 46, 58] the target: 1\n",
            "when input is [39, 59, 45, 46, 58, 1] the target: 46\n",
            "when input is [39, 59, 45, 46, 58, 1, 46] the target: 43\n",
            "when input is [39, 59, 45, 46, 58, 1, 46, 43] the target: 1\n",
            "when input is [49] the target: 43\n",
            "when input is [49, 43] the target: 57\n",
            "when input is [49, 43, 57] the target: 1\n",
            "when input is [49, 43, 57, 1] the target: 53\n",
            "when input is [49, 43, 57, 1, 53] the target: 50\n",
            "when input is [49, 43, 57, 1, 53, 50] the target: 42\n",
            "when input is [49, 43, 57, 1, 53, 50, 42] the target: 1\n",
            "when input is [49, 43, 57, 1, 53, 50, 42, 1] the target: 46\n",
            "when input is [52] the target: 41\n",
            "when input is [52, 41] the target: 47\n",
            "when input is [52, 41, 47] the target: 43\n",
            "when input is [52, 41, 47, 43] the target: 52\n",
            "when input is [52, 41, 47, 43, 52] the target: 58\n",
            "when input is [52, 41, 47, 43, 52, 58] the target: 1\n",
            "when input is [52, 41, 47, 43, 52, 58, 1] the target: 56\n",
            "when input is [52, 41, 47, 43, 52, 58, 1, 56] the target: 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAF4fu-I05NY",
        "outputId": "ff6e0d03-fb50-45c7-e6a5-e1cfbdebaf71"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a4ea4694d50>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IwEyQPd1E-2f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    # This is automatically called\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,L) tensor of integers (B=batch, T=time)\n",
        "        logits = self.token_embedding_table(idx) # (B,L,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            # logits has dimension (B,T,C)\n",
        "            # Pytorch \"wants\" logits to have Channels as second dimension ( :, C, :)\n",
        "            logits = logits.view(B*T,C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # genereate function for the model\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) tensor of indices\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Like using .forward with idx = idx and targets = None\n",
        "            logits, loss = self(idx)\n",
        "            # Choose the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, V)\n",
        "            # Softmax function to get probabilities from floats across the V dimension\n",
        "            probs = F.softmax(logits, dim=-1) # (B, V)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # 1 is Time dimension (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(len(vocabulary))\n",
        "B = 4\n",
        "T = 8\n",
        "x_b, y_b = get_batch('train',B, T) # Get 4 8grams!\n",
        "logits, loss = model(x_b, y_b)\n",
        "print('Shape of logits',logits.shape)\n",
        "print('loss',loss)\n",
        "# I choose 0 to be the first token (it is the new line idx)\n",
        "print(f'\\nLet\\'s see what the bigram model predicts first token \\'0\\' which means next line:{decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())}')\n",
        "print('\\nWhich is nice but it SUCKS. The reason for it is its not trained! Let\\'s train it.')"
      ],
      "metadata": {
        "id": "Mxq2gZhlp7_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c40ad9-f17d-4c71-cdfb-dd9777cb192f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of logits torch.Size([32, 65])\n",
            "loss tensor(4.6453, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Let's see what the bigram model predicts first token '0' which means next line:\n",
            "P-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3!dcb\n",
            "\n",
            "Which is nice but it SUCKS. The reason for it is its not trained! Let's train it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "for steps in range(1000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train',batch_size,block_size)\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    # zeroing all gradients from previous step\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    # this getting gradients for parameters\n",
        "    loss.backward()\n",
        "    # this uses gradients to update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
        "print('This looks better appearance wise but it\\'s still gibrish.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsK_qbZARfL0",
        "outputId": "bd97a64d-9dd6-4a49-d0a1-4c8e68d79c2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.707463264465332\n",
            "\n",
            "vLLko'TMyatyIoconxad.?-tNSqYPsx&bF.oiR;BD$dZBMZv'K f bR$mIKptRPly:AUC&$zLK,qUEy&Ay;ZxjKVhmrdagC-bTop\n",
            "This looks better appearance wise but it's still gibrish.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> A few words about the lines of the steps<h3>\n",
        "\n",
        "<ul>\n",
        "    <li><b><code>optimizer.zero_grad(set_to_none=True)</code>:</b> This resets gradients of all optimized tensors to zero. In PyTorch, gradients computed from each backward pass are accumulated (added up) to the previous values unless explicitly zeroed out. Therefore, I need to clear them out at the start of each training step, otherwise, I would be computing gradient w.r.t the wrong values. The argument <code>set_to_none=True</code> makes this operation more efficient by directly setting the gradients to None instead creating new 0 tensors to hodl the values.</li>\n",
        "    <li><b><code>loss.backward()</code>:</b> This line computes the gradient of the loss with respect to the parameters of the model using automatic differentiation. Essentially, it calculates how much each parameter contributed to the loss. The results (i.e., the gradients) are stored in the respective tensor's `.grad` attribute.</li>\n",
        "    <li><b><code>optimizer.step()</code>:</b> After calculating the gradients, we need to use them to update the parameters. `optimizer.step()` performs this parameter update based on the current gradient (stored in `.grad` attribute of a parameter) and the update rule defined by the specific optimizer being used. For example, if you're using Stochastic Gradient Descent (SGD), the step would involve subtracting the gradient times the learning rate from the current parameter value.</li>\n",
        "</ul>\n",
        "\n"
      ],
      "metadata": {
        "id": "zoOpnONFAHo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YACA8VZ2VrlL",
        "outputId": "bdb7c3e2-d76d-460b-b381-e1bf734dc768"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape, x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAOlG5-mWzpL",
        "outputId": "842cd7ee-02f2-49a1-e17c-ebcbd8e8d95d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 8, 2]),\n",
              " tensor([[[ 0.1808, -0.0700],\n",
              "          [-0.3596, -0.9152],\n",
              "          [ 0.6258,  0.0255],\n",
              "          [ 0.9545,  0.0643],\n",
              "          [ 0.3612,  1.1679],\n",
              "          [-1.3499, -0.5102],\n",
              "          [ 0.2360, -0.2398],\n",
              "          [-0.9211,  1.5433]],\n",
              " \n",
              "         [[ 1.3488, -0.1396],\n",
              "          [ 0.2858,  0.9651],\n",
              "          [-2.0371,  0.4931],\n",
              "          [ 1.4870,  0.5910],\n",
              "          [ 0.1260, -1.5627],\n",
              "          [-1.1601, -0.3348],\n",
              "          [ 0.4478, -0.8016],\n",
              "          [ 1.5236,  2.5086]],\n",
              " \n",
              "         [[-0.6631, -0.2513],\n",
              "          [ 1.0101,  0.1215],\n",
              "          [ 0.1584,  1.1340],\n",
              "          [-1.1539, -0.2984],\n",
              "          [-0.5075, -0.9239],\n",
              "          [ 0.5467, -1.4948],\n",
              "          [-1.2057,  0.5718],\n",
              "          [-0.5974, -0.6937]],\n",
              " \n",
              "         [[ 1.6455, -0.8030],\n",
              "          [ 1.3514, -0.2759],\n",
              "          [-1.5108,  2.1048],\n",
              "          [ 2.7630, -1.7465],\n",
              "          [ 1.4516, -1.5103],\n",
              "          [ 0.8212, -0.2115],\n",
              "          [ 0.7789,  1.5333],\n",
              "          [ 1.6097, -0.4032]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "# each line of each batch to be the mean of all previous lines of x tensor and across dimension 0\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, dim=0)\n",
        "xbow"
      ],
      "metadata": {
        "id": "jCyt0TeRWvtD",
        "outputId": "95644313-0c0f-4e5f-c1b8-4cbe2dce61b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 0.1735, -0.0649],\n",
              "         [ 0.1685,  0.3348],\n",
              "         [-0.1621,  0.1765],\n",
              "         [-0.2312, -0.0436],\n",
              "         [-0.1015, -0.2855],\n",
              "         [-0.2593, -0.1630],\n",
              "         [-0.3015, -0.2293]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.4985, -0.5395],\n",
              "         [ 0.4954,  0.3420],\n",
              "         [ 1.0623, -0.1802],\n",
              "         [ 1.1401, -0.4462],\n",
              "         [ 1.0870, -0.4071],\n",
              "         [ 1.0430, -0.1299],\n",
              "         [ 1.1138, -0.1641]]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "print('randomn\\n', x)\n",
        "print('\\n')\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "print('weights\\n', wei)\n",
        "print('\\n')\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "print('temporal weights\\n', wei)\n",
        "print('\\n')\n",
        "xbow2 = wei @ x # this is originally (T, T) @ (B, T, C) but @ mult. makes it (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "print(xbow2)\n",
        "#torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "id": "t5-dZymWXRac",
        "outputId": "21c0e394-bba4-4f21-ba82-d6c6597256e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "randomn\n",
            " tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 1.0101,  0.1215],\n",
            "         [ 0.1584,  1.1340],\n",
            "         [-1.1539, -0.2984],\n",
            "         [-0.5075, -0.9239],\n",
            "         [ 0.5467, -1.4948],\n",
            "         [-1.2057,  0.5718],\n",
            "         [-0.5974, -0.6937]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.3514, -0.2759],\n",
            "         [-1.5108,  2.1048],\n",
            "         [ 2.7630, -1.7465],\n",
            "         [ 1.4516, -1.5103],\n",
            "         [ 0.8212, -0.2115],\n",
            "         [ 0.7789,  1.5333],\n",
            "         [ 1.6097, -0.4032]]])\n",
            "\n",
            "\n",
            "weights\n",
            " tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "\n",
            "\n",
            "temporal weights\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "\n",
            "\n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print('wei\\n',wei)\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print('wei\\n',wei)\n",
        "xbow3 = wei @ x\n",
        "print('xbow3\\n',xbow3)\n",
        "torch.allclose(xbow, xbow3)\n",
        "print(x[0],xbow3[0])"
      ],
      "metadata": {
        "id": "F7hS9hyKcP9d",
        "outputId": "041184ab-4a64-4663-96e0-a05a25e667b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei\n",
            " tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "wei\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
            "xbow3\n",
            " tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.0894, -0.4926],\n",
            "         [ 0.1490, -0.3199],\n",
            "         [ 0.3504, -0.2238],\n",
            "         [ 0.3525,  0.0545],\n",
            "         [ 0.0688, -0.0396],\n",
            "         [ 0.0927, -0.0682],\n",
            "         [-0.0341,  0.1332]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.8173,  0.4127],\n",
            "         [-0.1342,  0.4395],\n",
            "         [ 0.2711,  0.4774],\n",
            "         [ 0.2421,  0.0694],\n",
            "         [ 0.0084,  0.0020],\n",
            "         [ 0.0712, -0.1128],\n",
            "         [ 0.2527,  0.2149]],\n",
            "\n",
            "        [[-0.6631, -0.2513],\n",
            "         [ 0.1735, -0.0649],\n",
            "         [ 0.1685,  0.3348],\n",
            "         [-0.1621,  0.1765],\n",
            "         [-0.2312, -0.0436],\n",
            "         [-0.1015, -0.2855],\n",
            "         [-0.2593, -0.1630],\n",
            "         [-0.3015, -0.2293]],\n",
            "\n",
            "        [[ 1.6455, -0.8030],\n",
            "         [ 1.4985, -0.5395],\n",
            "         [ 0.4954,  0.3420],\n",
            "         [ 1.0623, -0.1802],\n",
            "         [ 1.1401, -0.4462],\n",
            "         [ 1.0870, -0.4071],\n",
            "         [ 1.0430, -0.1299],\n",
            "         [ 1.1138, -0.1641]]])\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]]) tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "K9U8p-03f-gH",
        "outputId": "00001848-cc8d-4bde-f56f-6cd176e363b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XEfd7HNChXjK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}