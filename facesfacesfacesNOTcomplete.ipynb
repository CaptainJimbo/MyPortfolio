{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I would like to acknowledge the invaluable resources provided by MIT's Introduction to Deep Learning [website](http://introtodeeplearning.com). The videos and content available on the site played a crucial role in shaping my understanding of neural networks and deep learning and of course writing this notebook!"
      ],
      "metadata": {
        "id": "sepc-TCARnUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "        <a target=\"_blank\" href=\"https://sayhelloto.my.canva.site/dimitris-kogias\">\n",
        "        <img src=\"https://images.vexels.com/media/users/3/235238/isolated/lists/d2da2d8fdc12b538abac40c0f523e3e6-space-astronaut-badge.png\" alt=\"Website Logo\" style=\"border-radius:50%; padding: 5px;\" height=\"120px\" />\n",
        "        <br>\n",
        "        <center>\n",
        "        Visit my Website\n",
        "        </center>\n",
        "        </a>\n",
        "    </td><td></td><td></td><td></td>\n",
        "    <td>\n",
        "        <a target=\"_blank\" href=\"https://colab.research.google.com/github//CaptainJimbo/MyPortfolio/blob/main/facesfacesfacesNOTcomplete.ipynb\">\n",
        "            <img src=\"https://static-00.iconduck.com/assets.00/colab-icon-256x256-n6shvrxk.png\" alt=\"Colab Logo\" style=\"border-radius:50%; padding: 5px;\" height=\"120px\" />\n",
        "            <br>\n",
        "            <center>\n",
        "            Run in Google Colab\n",
        "            </center>\n",
        "        </a>\n",
        "    </td><td></td><td></td><td></td>\n",
        "    <td>\n",
        "        <a target=\"_blank\" href=\"https://github.com/CaptainJimbo/MyPortfolio/blob/main/facesfacesfacesNOTcomplete.ipynb\">\n",
        "            <img src=\"https://baionic.net/static/img/git.png\" alt=\"GitHub Logo\" style=\"border-radius:50%; padding: 5px;\" height=\"120px\" />\n",
        "            <br>\n",
        "            <center>\n",
        "            View Source on GitHub\n",
        "            </center>\n",
        "        </a>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "pGmZ6rNkOlb7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAfeStCT8kG4"
      },
      "source": [
        "<h1> Face Detection </h1>\n",
        "\n",
        "In this notebook I will build a DL model for facial detection.\n",
        "<ol>\n",
        " <li> I will build a standard CNN classifier to recognise if an image depicts a face or not. </li>\n",
        "\n",
        " <li> Afterwards, for reason that will unfold in the notebook, I will build SS-VAE to recognise faces while recognizing hidden biases in the image dataset.</li>\n",
        "</ol>\n",
        "\n",
        "**SS-VAE** is a semisupervised variational autoencoder model that learns *latent distribution* of features underlying face image datasets in order to [uncover hidden biases](http://introtodeeplearning.com/AAAI_MitigatingAlgorithmicBias.pdf)\n",
        "\n",
        "The **dataset** is comprised of images of 64x64x3 dimension of faces and not faces.\n",
        "\n",
        "1.   **Positive example training data (faces)**: [CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). Over 200K images of celebrity faces.   \n",
        "2.   **Negative example training data (not faces)**: [ImageNet](http://www.image-net.org/). Various images of different categories, using only non-human categories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-HZhNrh8nLj",
        "outputId": "f73aeb4e-f934-4f1f-ad3e-5a80646c0078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mitdeeplearning in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mitdeeplearning) (1.23.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from mitdeeplearning) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mitdeeplearning) (4.66.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from mitdeeplearning) (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->mitdeeplearning) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->mitdeeplearning) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "# Libraries Used\n",
        "\n",
        "import tensorflow as tf\n",
        "import IPython\n",
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import h5py\n",
        "\n",
        "\n",
        "#!pip install ipywidgets\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classes and defs\n",
        "\n",
        "class TrainingDatasetLoader(object):\n",
        "    def __init__(self, data_path):\n",
        "\n",
        "        print (\"Opening {}\".format(data_path))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        self.cache = h5py.File(data_path, 'r')\n",
        "\n",
        "        print (\"Loading data into memory...\")\n",
        "        sys.stdout.flush()\n",
        "        self.images = self.cache['images'][:]\n",
        "        self.labels = self.cache['labels'][:].astype(np.float32)\n",
        "        self.image_dims = self.images.shape\n",
        "        n_train_samples = self.image_dims[0]\n",
        "\n",
        "        self.train_inds = np.random.permutation(np.arange(n_train_samples))\n",
        "\n",
        "        self.pos_train_inds = self.train_inds[ self.labels[self.train_inds, 0] == 1.0 ]\n",
        "        self.neg_train_inds = self.train_inds[ self.labels[self.train_inds, 0] != 1.0 ]\n",
        "\n",
        "    def get_train_size(self):\n",
        "        return self.train_inds.shape[0]\n",
        "\n",
        "    def get_train_steps_per_epoch(self, batch_size, factor=10):\n",
        "        return self.get_train_size()//factor//batch_size\n",
        "\n",
        "    def get_batch(self, n, only_faces=False, p_pos=None, p_neg=None, return_inds=False):\n",
        "        if only_faces:\n",
        "            selected_inds = np.random.choice(self.pos_train_inds, size=n, replace=False, p=p_pos)\n",
        "        else:\n",
        "            selected_pos_inds = np.random.choice(self.pos_train_inds, size=n//2, replace=False, p=p_pos)\n",
        "            selected_neg_inds = np.random.choice(self.neg_train_inds, size=n//2, replace=False, p=p_neg)\n",
        "            selected_inds = np.concatenate((selected_pos_inds, selected_neg_inds))\n",
        "\n",
        "        sorted_inds = np.sort(selected_inds)\n",
        "        train_img = (self.images[sorted_inds,:,:,::-1]/255.).astype(np.float32)\n",
        "        train_label = self.labels[sorted_inds,...]\n",
        "        return (train_img, train_label, sorted_inds) if return_inds else (train_img, train_label)\n",
        "\n",
        "    def get_n_most_prob_faces(self, prob, n):\n",
        "        idx = np.argsort(prob)[::-1]\n",
        "        most_prob_inds = self.pos_train_inds[idx[:10*n:10]]\n",
        "        return (self.images[most_prob_inds,...]/255.).astype(np.float32)\n",
        "\n",
        "    def get_all_train_faces(self):\n",
        "        #return self.images[ self.pos_train_inds ]\n",
        "        return (self.images[ self.pos_train_inds,:,:,::-1]/255.).astype(np.float32)\n",
        "\n",
        "def show(images,n=16):\n",
        "    length = int(np.sqrt(n))\n",
        "    plt.figure(figsize=(round(length*3/2),round(length*3/2)))\n",
        "    random_inds = np.random.choice(len(images),n)\n",
        "    for i in range(length**2):\n",
        "        plt.subplot(length,length,i+1); plt.grid(False); plt.xticks([]); plt.yticks([])\n",
        "        image_ind = random_inds[i]\n",
        "        plt.imshow(images[image_ind])#, cmap=plt.cm.binary)\n",
        "        #plt.xlabel(image_ind)\n",
        "    plt.show()\n",
        "\n",
        "def plot_images(idx_face, idx_not_face):\n",
        "    plt.figure(figsize=(6,3))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(face_images[idx_face])\n",
        "    plt.title(\"Face\") ; plt.grid(False); plt.xticks([]); plt.yticks([])\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(not_face_images[idx_not_face])\n",
        "    plt.title(\"Not Face\"); plt.grid(False); plt.xticks([]); plt.yticks([])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "smc-FVm8bsx3"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXNQxLsM8yuT",
        "outputId": "aba4e358-d8ee-456e-881e-fc6b796d84f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening /root/.keras/datasets/train_face.h5\n",
            "Loading data into memory...\n"
          ]
        }
      ],
      "source": [
        "# The function tf.keras.utils.get_file() is a utility provided by TensorFlow's Keras API to download a file from a URL if it isn't\n",
        "# already in the cache. This can be useful for fetching datasets, model weights, or any other file necessary for an experiment.\n",
        "# The function will look for the file in the cache directory first (by default, ~/.keras/), and if it doesn't find it, it will download it.\n",
        "\n",
        "path_to_training_data = tf.keras.utils.get_file('train_face.h5', 'https://www.dropbox.com/s/hlz8atheyozp1yx/train_face.h5?dl=1')\n",
        "# Instantiate a TrainingDatasetLoader using the downloaded dataset\n",
        "loader = TrainingDatasetLoader(path_to_training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu-66ccX94ZG"
      },
      "outputs": [],
      "source": [
        "number_of_training_examples = loader.get_train_size()\n",
        "print(f'Training data has {number_of_training_examples} samples, labeled images of faces and not faces.')\n",
        "\n",
        "(images, labels) = loader.get_batch(1000)\n",
        "print(f'\\nI picked a random batch of {len(images)} faces and {len(images)} non-faces samples. Let\\'s scroll through 100 images from the batch\\n')\n",
        "\n",
        "face_images = images[np.where(labels==1)[0]]\n",
        "not_face_images = images[np.where(labels==0)[0]]\n",
        "\n",
        "face_slider = widgets.IntSlider(min=0, max=49, step=1, value=10)\n",
        "not_face_slider = widgets.IntSlider(min=0, max=49, step=1, value=12)\n",
        "widgets.interactive(plot_images, idx_face=face_slider, idx_not_face=not_face_slider)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Let\\'s view more faces. Looking at a bigger sample reveals something for the dataset...')\n",
        "print('\\nEven though this sample is randomly picked, most images (if not all) are of light skinned males and females. By law of big numbers this means that\\\n",
        " certain ethnicity groups \\nare overrepresented in the dataset. A classifier (like a CNN) can become algorithmically biased by this \\\n",
        "imbalance. For example the classifier could identify less \\naccurately dark skinned males and females).\\n')\n",
        "show(face_images,25)"
      ],
      "metadata": {
        "id": "gIKT3TbPz7x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXmD4fwEGjcs"
      },
      "source": [
        "<h1> Let's train a standard CNN to do classification. </h1>\n",
        "\n",
        "<justify>\n",
        "The CNN model is trained to predict if a specific image depicts a face or doesn't. It's architecture follows the typical pattern, with several <b> convolutional layers + batch normalizations, followed by two fully connected  layers, one for flattening of the convolution output a dense layer for class prediction to the single output.</b>.\n",
        "</justify>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlXbKWe6Lllc"
      },
      "outputs": [],
      "source": [
        "def CNN_classifier(n_outputs=1): # Output is a single logit indicating face or no face.\n",
        "\n",
        "    Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n",
        "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "    Flatten = tf.keras.layers.Flatten\n",
        "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "\n",
        "    # 1st CNN layer has 12 filters. For each filter and at each position, the kernel computes a weighted sum\n",
        "    # of the input pixels by convolution within its area and produces a single output pixel in the feature map.\n",
        "    # In our case, this means each filter in the first layer produces a 32x32 feature map.  (64/2=32)\n",
        "    Conv2D(filters=1*12, kernel_size=5,  strides=2), # Output is a tensor with shape (32,32,12)\n",
        "    BatchNormalization(), # BatchNorm would normalize each of the 12 channels (feature maps) based on the statistics of the current mini-batch.\n",
        "\n",
        "    #2nd CNN layer\n",
        "    Conv2D(filters=2*12, kernel_size=5,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    #3rd CNN layer\n",
        "    Conv2D(filters=4*12, kernel_size=3,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    #4th CNN layer\n",
        "    Conv2D(filters=6*12, kernel_size=3,  strides=2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    #5th layer\n",
        "    Flatten(), # This is how the algorithm \"sees\" the images.\n",
        "\n",
        "    #6th layer\n",
        "    Dense(512),\n",
        "\n",
        "    #7th layer\n",
        "    Dense(n_outputs, activation=None), # Output is by default 1 dimensional, no activation function means ouput is a logit.\n",
        "\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "standard_classifier = CNN_classifier()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "standard_classifier.predict(np.array([face_images[0]]))\n",
        "standard_classifier.summary()"
      ],
      "metadata": {
        "id": "VFZo-giDg9tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#some_faces = loader.get_batch(16+1)[0]\n",
        "#plt.figure(figsize=(6,6))\n",
        "#for i in range(16):\n",
        "#    plt.subplot(4,4,i+1)\n",
        "#    plt.xticks([])\n",
        "#    plt.yticks([])\n",
        "#   plt.grid(False)\n",
        "#    plt.imshow(some_faces[i])#, cmap=plt.cm.binary)\n",
        "#    plt.xlabel(round(standard_classifier.predict(np.array([some_faces[i]]))[0][0],3))\n",
        "#plt.show()\n",
        "#print('Some images and their predictions (single logits) using the untrained CNN.')"
      ],
      "metadata": {
        "id": "mRxhPFBHGygL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkbk4EE_M-Ik"
      },
      "outputs": [],
      "source": [
        "# Training of the CNN\n",
        "\n",
        "batch_size = 64\n",
        "num_epochs = 2\n",
        "learning_rate = 5e-4\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate) # Adam is a \"stochastic gradient descent type\" optimizer.\n",
        "# It change the weights of the innerconnected nodes in order to \"surf down\" the biggest descent in the multidimensional\n",
        "# weight n-plane.\n",
        "\n",
        "#loss_history = mdl.util.LossHistory(smoothing_factor=0.99) # to record loss evolution\n",
        "#plotter = mdl.util.PeriodicPlotter(sec=2, scale='semilogy')\n",
        "\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "\n",
        "@tf.function\n",
        "def standard_train_step(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = standard_classifier(x) # Output of classifer for batches x\n",
        "    loss = tf.nn.sigmoid_cross_entropy_with_logits(y,logits)\n",
        "\n",
        "  # Backpropagation\n",
        "  grads = tape.gradient(loss, standard_classifier.trainable_variables) # Derivative with respect to weights. (Multivariable and chained)\n",
        "  optimizer.apply_gradients(zip(grads, standard_classifier.trainable_variables)) # Sliding to the lowest point of an n-parameter landscape.\n",
        "  return loss\n",
        "\n",
        "# The training loop.\n",
        "for epoch in range(num_epochs):\n",
        "    #for idx in tqdm(range(loader.get_train_size()//batch_size)):\n",
        "    # I used less iterations as it takes far lesser time and it is more than enough\n",
        "    for idx in tqdm(range(10000//batch_size),desc=f'Epoch {epoch+1}'):\n",
        "        # Grab a batch of training data and propagate through our network\n",
        "        x, y = loader.get_batch(batch_size)\n",
        "        loss = standard_train_step(x, y)\n",
        "\n",
        "        # Record the loss and plot the evolution of the loss as a function of training\n",
        "        #loss_history.append(loss.numpy().mean())\n",
        "        #plotter.plot(loss_history.get())\n",
        "\n",
        "print('Loss = ',loss.numpy().mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJwFzD6DN2mc"
      },
      "outputs": [],
      "source": [
        "# Pick more samples for testing. I can do that since total dataset size is 100k\n",
        "(batch_x, batch_y) = loader.get_batch(5000)\n",
        "y_pred_standard = tf.round(tf.nn.sigmoid(standard_classifier.predict(batch_x))) # there is no activation function at CNN class I built\n",
        "# tf.equal() compares\n",
        "# tf.cast() transforms to type ( ,)\n",
        "# tf.reduce_mean() find the average\n",
        "acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
        "\n",
        "print(\"Standard CNN accuracy on (potentially biased) training set: {:.4f}\".format(acc_standard.numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<p align='justify'>\n",
        "<center><img src=\"https://i.imgflip.com/7vsslk.jpg\n",
        "\"  width=\"500\"></center>\n",
        "\n",
        "This is a good (excellent) performance. However the nn algorithm distinguishes faces from non faces by finding latent characteristics (features) inside the pictures that we can't recognise at first glance. It's also important to remember that just because a neural network perceives more characteristics doesn't mean it's infallible. Since we can not perceive such characteristics and we are the ones who picked the sample, it is very probable that such characteristics are not equally stratified among the dataset and meaning it is inbalanced. This can lead to the training of a model that is algorithmically biased.\n",
        "</p>\n",
        "\n",
        "\n",
        "<h1> SOLUTIONS </h1>\n",
        "\n",
        "<p align='justify'> <center><img src=\"https://www.thoughtco.com/thmb/DlzsYR0whBb3taEl45YTviOZtxM=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/GettyImages-11410491702-07f03ccd0eb54d36a9b62212cd2f55a1.jpg\" width=\"400\"></center>\n",
        "\n",
        "A naive first approach (most usually done) would be to create different subclasses (i.e., light-skinned females, males with hats, etc.) within the training data, and then evaluate classifier performance with respect to these clustered groups. But there are two big problems with this. First, labeling lots of data that way takes a lot of time and effort. Second, we might miss out on some biases because we didnâ€™t think of them when labeling like race, hats, glasses, etc.\n",
        "A way to explain this metaphorically is by the allegory of Plato's Cave, where the prisoners can only recognise the shapes of objects while a freed man recognises more quintessential properties. In that case a prisoner and freed man would probably have different ways of recognizing objects and divide them in different groups. So in this case, we are the prisoners and the algorithm is the freed men! <br> Imbalances in the training data can result in unwanted algorithmic bias. For example, the majority of faces in the training dataset are those of light-skinned females. As a result, a classifier trained that will be better suited at recognizing and classifying faces with features similar to these, and will thus be biased, more technically the classiers' classification decision changes after it sees some additional latent features or variables. <br> **But is there a way to \"learn\" the distribution of these latent characteristics in an unbiased, unsupervised manner, without the need for any annotation and grouping? We can use VAEs!**\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "yfMldM4TSiiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"text-align: center\"> <h1> The concept of VAEs</h1></div>\n",
        "\n",
        "<center> <img src=\"https://i.ibb.co/3s4S6Gc/vae.jpg\n",
        "\"  width=\"700\"/>\n",
        "</center>\n",
        "\n",
        "<justify>\n",
        "The equation for the latent loss is provided by:\n",
        "\n",
        "$$L_{KL}(\\mu, \\sigma) = \\frac{1}{2}\\sum_{j=0}^{k-1} (\\sigma_j + \\mu_j^2 - 1 - \\log{\\sigma_j})$$\n",
        "\n",
        "The equation for the reconstruction loss is provided by:\n",
        "\n",
        "$$L_{x}{(x,\\hat{x})} = ||x-\\hat{x}||_1$$\n",
        "\n",
        "Thus for the VAE loss we have:\n",
        "\n",
        "$$L_{VAE} = c\\cdot L_{KL} + L_{x}{(x,\\hat{x})}$$\n",
        "\n",
        "where $c$ is a weighting coefficient used for regularization. Now we're ready to define our VAE loss function:\n",
        "</justify>"
      ],
      "metadata": {
        "id": "xzJZJHrEkuX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ARGUMENTS\n",
        "# x : the true input\n",
        "# x_recon : the iput that is encoded and then decoded using the VAE\n",
        "# mu, logsigma : latent distribution characteristics\n",
        "\n",
        "# RETURNS\n",
        "# vae loss\n",
        "\n",
        "def vae_loss_function(x, x_recon, mu, logsigma, kl_weight=0.0005):\n",
        "\n",
        "  latent_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1.0 - logsigma, axis=1)\n",
        "  reconstruction_loss = tf.reduce_mean(tf.abs(x-x_recon), axis=(1,2,3))\n",
        "  vae_loss = kl_weight * latent_loss + reconstruction_loss\n",
        "\n",
        "  return vae_loss"
      ],
      "metadata": {
        "id": "y23fqh-uT1p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "VAEs use a \"reparameterization  trick\" for sampling learned latent variables. The VAE encoder generates a vector of means and a vector of standard deviations that are constrained to roughly follow Gaussian distributions. We then sample a noise value $\\epsilon$ from a Gaussian distribution, and then scale it by the standard deviation and add back the mean to output the result as our sampled latent vector. Formalizing this for a latent variable $z$ where we sample $\\epsilon \\sim N(0,(I))$ we have:\n",
        "\n",
        "$$z = \\mu + e^{\\left(\\frac{1}{2} \\cdot \\log{\\Sigma}\\right)}\\circ \\epsilon$$\n",
        "\n",
        "where $\\mu$ is the mean and $\\Sigma$ is the covariance matrix. This is useful because it will let us neatly define the loss function for the VAE, generate randomly sampled latent variables, achieve improved network generalization, **and** make our complete VAE network differentiable so that it can be trained via backpropagation. Quite powerful!\n",
        "\n",
        "Let's define a function to implement the VAE sampling operation:"
      ],
      "metadata": {
        "id": "6mbUOjrEkwUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ARGUMENTS\n",
        "# z_mean\n",
        "# z_logsigma\n",
        "\n",
        "# RETURNS\n",
        "# z:\n",
        "\n",
        "def sampling(z_mean, z_logsigma):\n",
        "  # random.normal is mean=0 and std=1.0\n",
        "  batch, latent_dim = z_mean.shape\n",
        "  epsilon = tf.random.normal(shape=(batch, latent_dim))\n",
        "  z = z_mean + tf.math.exp(0.5 * z_logsigma) * epsilon\n",
        "  return z"
      ],
      "metadata": {
        "id": "ZcsaJpoyPSjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semi-supervised variational autoencoder (SS-VAE)\n",
        "\n",
        "A VAE that has a supervised component in order to both output a classification decision for the facial detection task and analyze where the biases in our model may be resulting from. While previous works like that of  Buolamwini and Gebru have focused on skin tone and gender as two categories where facial detection models may be experiencing bias, there may be other unlabeled features that also are biased, resulting in poorer classification performance. We will build our semi-supervised VAE (SS-VAE) to learn these underlying latent features.\n",
        "\n",
        "A general schematic of the SS-VAE architecture is shown here.\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab2/img/SS-VAE.png\"  width=\"600\"/>\n",
        "</center>\n",
        "\n",
        "We will apply our SS-VAE to a *supervised classification* problem -- the facial detection task. Importantly, note how the encoder portion in the SS-VAE architecture also outputs a single supervised variable, $z_o$, corresponding to the class prediction -- face or not face. Usually, VAEs are not trained to output any supervised variables (such as a class prediction)! This is the key distinction between the SS-VAE and a traditional VAE.\n",
        "\n",
        "Keep in mind that we only want to learn the latent representation of *faces*, as that is where we are interested in uncovering potential biases, even though we are training a model on a binary classification problem. So, we will need to ensure that, **for faces**, our SS-VAE model both learns a representation of the unsupervised latent variables, captured by the distribution $q_\\phi(z|x)$, and outputs a supervised class prediction $z_o$, but that, **for negative examples**, it only outputs a class prediction $z_o$."
      ],
      "metadata": {
        "id": "1gbHAiVrPwF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the SS-VAE loss function\n",
        "\n",
        "This means we'll need to be a bit clever about the loss function for the SS-VAE. The form of the loss will depend on whether it's a face image or a non-face image that's being considered.\n",
        "\n",
        "For **face images**, our loss function will have two components:\n",
        "\n",
        "1.   **VAE loss ($L_{VAE}$)**: consists of the latent loss and the reconstruction loss.\n",
        "2.   **Classification loss ($L_y(y,\\hat{y})$)**: standard cross-entropy loss for a binary classification problem.\n",
        "\n",
        "In contrast, for images of **non-faces**, our loss function is solely the classification loss.\n",
        "\n",
        "We can write a single expression for the loss by defining an indicator variable ${I}_f$which reflects which training data are images of faces (${I}_f(y) = 1$ ) and which are images of non-faces (${I}_f(y) = 0$). Using this, we obtain:\n",
        "\n",
        "$$L_{total} = L_y(y,\\hat{y}) + {I}_f(y)\\Big[L_{VAE}\\Big]$$\n",
        "\n",
        "Let's write a function to define the SS-VAE loss function:\n"
      ],
      "metadata": {
        "id": "JTxvTvSlQHT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ARGUMENTS\n",
        "# x: true input x\n",
        "# x_pred: reconstructed x\n",
        "# y: true label (face or not face)\n",
        "# y_logit: predicted labels\n",
        "# mu: mean of latent distribution (Q(z|X))\n",
        "# logsigma: log of standard deviation of latent distribution (Q(z|X))\n",
        "\n",
        "# RETURNS\n",
        "# total_loss: SS-VAE total loss\n",
        "# classification_loss: SS-VAE classification loss\n",
        "\n",
        "def ss_vae_loss_function(x, x_pred, y, y_logit, mu, logsigma):\n",
        "    vae_loss = vae_loss_function(x, x_pred, mu, logsigma)\n",
        "    classification_loss = tf.nn.sigmoid_cross_entropy_with_logits(y,y_logit)\n",
        "    face_indicator = tf.cast(tf.equal(y, 1), tf.float32) # This is I_f(y) = 1 or 0\n",
        "\n",
        "    # tf.reduce_mean to average over all samples\n",
        "    total_loss = tf.reduce_mean(classification_loss + face_indicator * vae_loss)\n",
        "    return total_loss, classification_loss, vae_loss"
      ],
      "metadata": {
        "id": "1FSfMbsLtBR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align='justify'>\n",
        "Defining the SS-VAE architecture\n",
        "Now we're ready to define the SS-VAE architecture. To build the SS-VAE, we will use the standard CNN classifier from above as our encoder, and then define a decoder network. We will create and initialize the encoder and decoder networks, and then construct the end-to-end VAE. We will use a latent space with 32 latent variables.\n",
        "\n",
        "The decoder network will take as input the sampled latent variables, run them through a series of deconvolutional layers, and output a reconstruction of the original input image. </p>"
      ],
      "metadata": {
        "id": "9-o03crc0aXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### DECODER PART OF SS-VAE ###\n",
        "\n",
        "def make_face_decoder_network(n_filters=12):\n",
        "\n",
        "    # Functionally define the different layer types we will use\n",
        "    Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu')\n",
        "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "    Flatten = tf.keras.layers.Flatten\n",
        "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
        "    Reshape = tf.keras.layers.Reshape\n",
        "\n",
        "\n",
        "    decoder = tf.keras.Sequential([\n",
        "    # Transform to pre-convolutional generation\n",
        "    Dense(units=4*4*6*n_filters),  # 4x4 feature maps (with 6N occurances)\n",
        "\n",
        "    Reshape(target_shape=(4, 4, 6*n_filters)),\n",
        "\n",
        "    # Upscaling convolutions (inverse of encoder before)\n",
        "    Conv2DTranspose(filters=4*n_filters, kernel_size=3,  strides=2),\n",
        "    Conv2DTranspose(filters=2*n_filters, kernel_size=3,  strides=2),\n",
        "    Conv2DTranspose(filters=1*n_filters, kernel_size=5,  strides=2),\n",
        "    Conv2DTranspose(filters=3, kernel_size=5,  strides=2),\n",
        "    ])\n",
        "\n",
        "    return decoder"
      ],
      "metadata": {
        "id": "V4HQGRmV0bB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's put encoder and decoder together. ##"
      ],
      "metadata": {
        "id": "JP6WpRY02zWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Defining and creating the SS-VAE ###\n",
        "\n",
        "class SS_VAE(tf.keras.Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(SS_VAE, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "    # Define the number of outputs for the encoder. Recall that we have\n",
        "    # `latent_dim` latent variables, as well as a supervised output for the\n",
        "    # classification.\n",
        "    num_encoder_dims = 2*self.latent_dim + 1\n",
        "\n",
        "    self.encoder = standard_CNN(num_encoder_dims)\n",
        "    self.decoder = make_face_decoder_network()\n",
        "\n",
        "  # function to feed images into encoder, encode the latent space, and output\n",
        "  #   classification probability\n",
        "  def encode(self, x):\n",
        "    # encoder output\n",
        "    encoder_output = self.encoder(x)\n",
        "\n",
        "    # classification prediction\n",
        "    y_logit = tf.expand_dims(encoder_output[:, 0], -1)\n",
        "    # latent variable distribution parameters\n",
        "    z_mean = encoder_output[:, 1:self.latent_dim+1]\n",
        "    z_logsigma = encoder_output[:, self.latent_dim+1:]\n",
        "\n",
        "    return y_logit, z_mean, z_logsigma\n",
        "\n",
        "  # Decode the latent space and output reconstruction\n",
        "  def decode(self, z):\n",
        "    # TODO: use the decoder (self.decoder) to output the reconstruction\n",
        "    reconstruction = self.decoder(z)\n",
        "    # reconstruction = # TODO\n",
        "    return reconstruction\n",
        "\n",
        "  # The call function will be used to pass inputs x through the core VAE\n",
        "  def call(self, x):\n",
        "    # Encode input to a prediction and latent space\n",
        "    y_logit, z_mean, z_logsigma = self.encode(x)\n",
        "\n",
        "    z = sampling(z_mean, z_logsigma)\n",
        "\n",
        "    recon = self.decode(z)\n",
        "\n",
        "    return y_logit, z_mean, z_logsigma, recon\n",
        "\n",
        "  # Predict face or not face logit for given input x\n",
        "  def predict(self, x):\n",
        "    y_logit, z_mean, z_logsigma = self.encode(x)\n",
        "    return y_logit\n",
        "\n",
        "ss_vae = SS_VAE(latent_dim=32)"
      ],
      "metadata": {
        "id": "bLhwyt1cbVkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Training the SS-VAE ###\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 5e-4\n",
        "latent_dim = 32\n",
        "\n",
        "# SS-VAE needs slightly more epochs to train since its more complex than\n",
        "# the standard classifier so we use 6 instead of 2\n",
        "num_epochs = 6\n",
        "\n",
        "# instantiate a new SS-VAE model and optimizer\n",
        "ss_vae = SS_VAE(latent_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# To define the training operation, we will use tf.function which is a powerful tool\n",
        "#   that lets us turn a Python function into a TensorFlow computation graph.\n",
        "@tf.function\n",
        "def ss_vae_train_step(x, y):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_logit, z_mean, z_logsigma, x_recon = ss_vae(x)\n",
        "        loss, class_loss, _ = ss_vae_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma)\n",
        "\n",
        "    grads = tape.gradient(loss, ss_vae.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, ss_vae.trainable_variables))\n",
        "\n",
        "    return loss\n",
        "\n",
        "# get training faces from data loader\n",
        "all_faces = loader.get_all_train_faces()\n",
        "\n",
        "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
        "\n",
        "# The training loop -- outer loop iterates over the number of epochs\n",
        "for i in range(num_epochs):\n",
        "\n",
        "  IPython.display.clear_output(wait=True)\n",
        "  print(\"Starting epoch {}/{}\".format(i+1, num_epochs))\n",
        "\n",
        "  # get a batch of training data and compute the training step\n",
        "  #for j in tqdm(range(10000 // batch_size)):\n",
        "  for j in tqdm(range(loader.get_train_size() // batch_size)):\n",
        "    # load a batch of data\n",
        "    (x, y) = loader.get_batch(batch_size)\n",
        "    # loss optimization\n",
        "    loss = ss_vae_train_step(x, y)\n",
        "\n",
        "    # plot the progress every 200 steps\n",
        "    if j % 500 == 0:\n",
        "      mdl.util.plot_sample(x, y, ss_vae)"
      ],
      "metadata": {
        "id": "ItqZn1hhbmR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Linking model performance to uncertainty and bias\n",
        "\n",
        "# Load a random sample of 5000 faces from our dataset and compute the model performance on them\n",
        "(x, y) = loader.get_batch(5000, only_faces=True)\n",
        "y_logit, z_mean, z_logsigma, x_recon = ss_vae(x)\n",
        "loss, class_loss, vae_loss = ss_vae_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma)\n",
        "\n",
        "# Sort the results by the vae loss scores\n",
        "vae_loss = vae_loss.numpy()\n",
        "ind = np.argsort(vae_loss, axis=None)\n",
        "\n",
        "# Plot the 25 samples with the highest and lowest reconstruction losses\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
        "ax[0].imshow(mdl.util.create_grid_of_images(x[ind[:25]]))\n",
        "ax[0].set_title(\"Samples with the lowest reconstruction loss \\n\" +\n",
        "                f\"Average recon loss: {np.mean(vae_loss[ind[:25]]):.2f}\")\n",
        "\n",
        "ax[1].imshow(mdl.util.create_grid_of_images(x[ind[-25:]]))\n",
        "ax[1].set_title(\"Samples with the highest reconstruction loss \\n\" +\n",
        "                f\"Average recon loss: {np.mean(vae_loss[ind[-25:]]):.2f}\");"
      ],
      "metadata": {
        "id": "UhL8W7epcAni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact\n",
        "\n",
        "def inspect_latent_features(idx_latent=21):\n",
        "    num_steps = 15\n",
        "\n",
        "    # Extract all latent samples from the desired dimension\n",
        "    latent_samples = z_mean[:, idx_latent]\n",
        "\n",
        "    # Compute their density and plot\n",
        "    density, latent_bins = np.histogram(latent_samples, num_steps, density=True)\n",
        "    fig, ax = plt.subplots(2, 1, figsize=(15, 4))\n",
        "    ax[0].bar(latent_bins[1:], density)\n",
        "    ax[0].set_ylabel(\"Data density\")\n",
        "\n",
        "    # Visualize reconstructions as we walk across the latent space\n",
        "    latent_steps = np.linspace(np.min(latent_samples), np.max(latent_samples), num_steps)\n",
        "    baseline_latent = tf.reduce_mean(z_mean, 0, keepdims=True)\n",
        "\n",
        "    recons = []\n",
        "    for step in latent_steps:\n",
        "        # Adjust the latent vector according to our step\n",
        "        latent = baseline_latent.numpy()\n",
        "        latent[0, idx_latent] = step\n",
        "        # Decode the reconstruction and store\n",
        "        recons.append(ss_vae.decode(latent)[0])\n",
        "\n",
        "    # Visualize all of the reconstructions!\n",
        "    ax[1].imshow(mdl.util.create_grid_of_images(recons, (1, num_steps)))\n",
        "    ax[1].set_xlabel(\"Latent step\")\n",
        "    ax[1].set_ylabel(\"Visualization\")\n",
        "    plt.show()\n",
        "\n",
        "# Create an interactive slider below the cell\n",
        "interact(inspect_latent_features, idx_latent=(0, 31, 1));\n"
      ],
      "metadata": {
        "id": "obEUWbQ4mxbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Accuracy vs. density in latent space\n",
        "\n",
        "# Loop through every latent dimension\n",
        "avg_logit_per_bin = []\n",
        "for idx_latent in range(latent_dim):\n",
        "  latent_samples = z_mean[:, idx_latent]\n",
        "  start = np.percentile(latent_samples, 5)\n",
        "  end = np.percentile(latent_samples, 95)\n",
        "  latent_steps = np.linspace(start, end, num_steps)\n",
        "\n",
        "  # Find which samples fall in which bin of the latent dimension\n",
        "  which_latent_bin = np.digitize(latent_samples, latent_steps)\n",
        "\n",
        "  # For each latent bin, compute the accuracy (average logit score)\n",
        "  avg_logit = []\n",
        "  for j in range(0, num_steps+1):\n",
        "    inds_in_bin = np.where(which_latent_bin == j)\n",
        "    avg_logit.append(y_logit.numpy()[inds_in_bin].mean())\n",
        "\n",
        "  avg_logit_per_bin.append(avg_logit)\n",
        "\n",
        "# Average the results across all latent dimensions and all samples\n",
        "accuracy_per_latent = np.mean(avg_logit_per_bin, 0)\n",
        "accuracy_per_latent = (accuracy_per_latent - accuracy_per_latent.min()) / np.ptp(accuracy_per_latent)\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(np.linspace(np.min(z_mean), np.max(z_mean), num_steps+1), accuracy_per_latent,'-o')\n",
        "plt.xlabel(\"Latent step\")\n",
        "plt.ylabel(\"Relative accuracy\")\n",
        ""
      ],
      "metadata": {
        "id": "W3L_FM_fuREj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3d6UFAFRuVtQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWOGtS7mF3qYC6ipCOvqdD"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}